{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_dir = '/Users/lakke21/Downloads/summarization-aacl/abstractive/Legal-Pegasus/output'\n",
    "reference_dir = '/Users/lakke21/Downloads/dataset/IN-Abs/test-data/summary'\n",
    "all_scores = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for 6728.txt: [{'rouge-1': {'r': 0.44607843137254904, 'p': 0.5870967741935483, 'f': 0.5069637833939837}, 'rouge-2': {'r': 0.268361581920904, 'p': 0.36964980544747084, 'f': 0.3109656252405839}, 'rouge-l': {'r': 0.43137254901960786, 'p': 0.567741935483871, 'f': 0.4902506914719781}}]\n",
      "Scores for 6270.txt: [{'rouge-1': {'r': 0.4425287356321839, 'p': 0.48427672955974843, 'f': 0.4624624574726077}, 'rouge-2': {'r': 0.19314641744548286, 'p': 0.22382671480144403, 'f': 0.20735785455884173}, 'rouge-l': {'r': 0.41379310344827586, 'p': 0.4528301886792453, 'f': 0.4324324274425777}}]\n",
      "Scores for 1697.txt: [{'rouge-1': {'r': 0.4835680751173709, 'p': 0.4801864801864802, 'f': 0.4818713400293013}, 'rouge-2': {'r': 0.23649337410805302, 'p': 0.24919441460794844, 'f': 0.2426778192712018}, 'rouge-l': {'r': 0.4154929577464789, 'p': 0.4125874125874126, 'f': 0.41403508271935985}}]\n",
      "Metric     | Recall     | Precision  | F-Score   \n",
      "--------------------------------------------------\n",
      "rouge-1    | 0.48       | 0.48       | 0.48      \n",
      "rouge-2    | 0.24       | 0.25       | 0.24      \n",
      "rouge-l    | 0.42       | 0.41       | 0.41      \n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "hypothesis_files = os.listdir(hypothesis_dir)\n",
    "reference_files = os.listdir(reference_dir)\n",
    "\n",
    "# Match and compare files\n",
    "for ref_file in reference_files:\n",
    "    # Construct the corresponding reference file name\n",
    "    # This assumes the files have the same names in both folders\n",
    "    hyp_file = ref_file\n",
    "\n",
    "    # Ensure the reference file exists\n",
    "    if hyp_file in hypothesis_files:\n",
    "        with open(os.path.join(hypothesis_dir, hyp_file), 'r') as hyp_f, open(os.path.join(reference_dir, ref_file), 'r') as ref_f:\n",
    "            hyp_text = hyp_f.read()\n",
    "            ref_text = ref_f.read()\n",
    "\n",
    "            # Calculate ROUGE scores\n",
    "            scores = rouge.get_scores(hyp_text, ref_text)\n",
    "            #df_scores = pd.DataFrame(scores).T  # Transpose to get metrics as rows\n",
    "            #df_scores.columns = ['F-score', 'Precision', 'Recall']\n",
    "            #print(df_scores)\n",
    "            print(f\"Scores for {hyp_file}: {scores}\")\n",
    "first_score_set = scores[0]\n",
    "\n",
    "# Print the header\n",
    "print(f\"{'Metric':<10} | {'Recall':<10} | {'Precision':<10} | {'F-Score':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print each ROUGE metric with its scores\n",
    "for key, value in first_score_set.items():\n",
    "    recall = value['r']\n",
    "    precision = value['p']\n",
    "    f_score = value['f']\n",
    "    print(f\"{key:<10} | {recall:<10.2f} | {precision:<10.2f} | {f_score:<10.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
