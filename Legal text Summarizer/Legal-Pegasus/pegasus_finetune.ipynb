{"cells":[{"cell_type":"markdown","id":"6cf3e374","metadata":{"id":"6cf3e374"},"source":["### Script to finetune pegasus model\n","source https://gist.github.com/jiahao87/50cec29725824da7ff6dd9314b53c4b3\n","\n","add the path to the fine tuning data excel to filename variable"]},{"cell_type":"code","execution_count":8,"id":"670aac2b","metadata":{"id":"670aac2b"},"outputs":[{"data":{"text/plain":["100"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","filename = \"/Users/lakke21/Downloads/summarization-aacl/abstractive/Legal-Pegasus/exceldataset.xlsx\"\n","\n","df = pd.read_excel(filename)\n","\n","#df.rename(columns = {'data':'source', 'summary':'target'}, inplace = True)\n","len(df)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Special Leave Petition Nos.\\n823 24 of 1990.\\n...</td>\n","      <td>Petitioners ' lands were acquired by the respo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ivil Appeal No. 4649 of 1989.\\nFrom the Judgme...</td>\n","      <td>Pursuant to a scheme enacted for the benefit o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Appeals, Nos. 275 276 of 1963.\\nAppeals by spe...</td>\n","      <td>By section 25 (4) of the Income tax Act, \"Wher...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>No. 7338 of 1981.\\n(Under Article 32 of the Co...</td>\n","      <td>Fundamental Rule 56(j) confers power on the ap...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(C) No. 677 of 1988.\\n(Under Article 32 of the...</td>\n","      <td>The Lt. Governor of Delhi amended the Delhi Po...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              source  \\\n","0  Special Leave Petition Nos.\\n823 24 of 1990.\\n...   \n","1  ivil Appeal No. 4649 of 1989.\\nFrom the Judgme...   \n","2  Appeals, Nos. 275 276 of 1963.\\nAppeals by spe...   \n","3  No. 7338 of 1981.\\n(Under Article 32 of the Co...   \n","4  (C) No. 677 of 1988.\\n(Under Article 32 of the...   \n","\n","                                              target  \n","0  Petitioners ' lands were acquired by the respo...  \n","1  Pursuant to a scheme enacted for the benefit o...  \n","2  By section 25 (4) of the Income tax Act, \"Wher...  \n","3  Fundamental Rule 56(j) confers power on the ap...  \n","4  The Lt. Governor of Delhi amended the Delhi Po...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":10,"id":"ad95a5e7","metadata":{"id":"ad95a5e7"},"outputs":[],"source":["from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments, PegasusTokenizerFast\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","\n","class PegasusDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n","        return item\n","    def __len__(self):\n","        return len(self.labels['input_ids'])  # len(self.labels)\n","\n","      \n","def prepare_data(model_name, \n","                 train_texts, train_labels, \n","                 val_texts=None, val_labels=None, \n","                 test_texts=None, test_labels=None):\n","  \"\"\"\n","  Prepare input data for model fine-tuning\n","  \"\"\"\n","  tokenizer = PegasusTokenizerFast.from_pretrained(model_name)\n","\n","  prepare_val = False if val_texts is None or val_labels is None else True\n","  prepare_test = False if test_texts is None or test_labels is None else True\n","\n","  def tokenize_data(texts, labels):\n","    encodings = tokenizer(texts, truncation=True, padding=True, max_length = 512)\n","    decodings = tokenizer(labels, truncation=True, padding=True, max_length = 256)\n","    dataset_tokenized = PegasusDataset(encodings, decodings)\n","    return dataset_tokenized\n","\n","  train_dataset = tokenize_data(train_texts, train_labels)\n","  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n","  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n","\n","  return train_dataset, val_dataset, test_dataset, tokenizer\n","\n","\n","def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n","  \"\"\"\n","  Prepare configurations and base model for fine-tuning\n","  \"\"\"\n","  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n","\n","  if freeze_encoder:\n","    for param in model.model.encoder.parameters():\n","      param.requires_grad = False\n","\n","  if val_dataset is not None:\n","    training_args = TrainingArguments(\n","      output_dir=output_dir,           # output directory\n","      num_train_epochs=2,           # total number of training epochs\n","      per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n","      per_device_eval_batch_size=1,    # batch size for evaluation, can increase if memory allows\n","      save_steps=500,                  # number of updates steps before checkpoint saves\n","      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n","      evaluation_strategy='steps',     # evaluation strategy to adopt during training\n","      eval_steps=100,                  # number of update steps before evaluation\n","      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","      weight_decay=0.01,               # strength of weight decay\n","      logging_dir='./logs',            # directory for storing logs\n","      logging_steps=100,\n","    )\n","\n","    trainer = Trainer(\n","      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","      args=training_args,                  # training arguments, defined above\n","      train_dataset=train_dataset,         # training dataset\n","      eval_dataset=val_dataset,            # evaluation dataset\n","      tokenizer=tokenizer\n","    )\n","\n","  else:\n","    training_args = TrainingArguments(\n","      output_dir=output_dir,           # output directory\n","      num_train_epochs=2,           # total number of training epochs\n","      per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n","      save_steps=500,                  # number of updates steps before checkpoint saves\n","      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n","      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","      weight_decay=0.01,               # strength of weight decay\n","      logging_dir='./logs',            # directory for storing logs\n","      logging_steps=100,\n","    )\n","\n","    trainer = Trainer(\n","      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","      args=training_args,                  # training arguments, defined above\n","      train_dataset=train_dataset,         # training dataset\n","      tokenizer=tokenizer\n","    )\n","\n","  return trainer"]},{"cell_type":"code","execution_count":11,"id":"086e44a4","metadata":{"colab":{"background_save":true},"id":"086e44a4"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a0b69eb56b14033acbc487e5c327d7b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 3.079, 'learning_rate': 1e-05, 'epoch': 1.0}\n","{'loss': 2.9653, 'learning_rate': 2e-05, 'epoch': 2.0}\n","{'train_runtime': 4053.1709, 'train_samples_per_second': 0.049, 'train_steps_per_second': 0.049, 'train_loss': 3.0221885681152343, 'epoch': 2.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=200, training_loss=3.0221885681152343, metrics={'train_runtime': 4053.1709, 'train_samples_per_second': 0.049, 'train_steps_per_second': 0.049, 'train_loss': 3.0221885681152343, 'epoch': 2.0})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","train_texts, train_labels = (list(df['source'])), (list(df['target']))\n","  \n","model_name = 'nsi319/legal-pegasus'\n","train_dataset, _, _, tokenizer = prepare_data(model_name, train_texts, train_labels)\n","trainer = prepare_fine_tuning(model_name, tokenizer, train_dataset)\n","trainer.train()"]},{"cell_type":"code","execution_count":12,"id":"PtPGCoHxfXGv","metadata":{"colab":{"background_save":true},"id":"PtPGCoHxfXGv"},"outputs":[{"data":{"text/plain":["('./ouput_model/tokenizer_config.json',\n"," './ouput_model/special_tokens_map.json',\n"," './ouput_model/spiece.model',\n"," './ouput_model/added_tokens.json',\n"," './ouput_model/tokenizer.json')"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","if not os.path.exists('./ouput_model/'):\n","    os.makedirs('./ouput_model/')\n","trainer.model.save_pretrained(\"./ouput_model/\")\n","tokenizer.save_pretrained(\"./ouput_model/\")\n"]},{"cell_type":"code","execution_count":13,"id":"h6qzOTIVhQVM","metadata":{"colab":{"background_save":true},"id":"h6qzOTIVhQVM"},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: ouput_model/ (stored 0%)\n","  adding: ouput_model/tokenizer_config.json (deflated 94%)\n","  adding: ouput_model/special_tokens_map.json (deflated 82%)\n","  adding: ouput_model/config.json (deflated 61%)\n","  adding: ouput_model/tokenizer.json (deflated 78%)\n","  adding: ouput_model/generation_config.json (deflated 44%)\n","  adding: ouput_model/spiece.model (deflated 50%)\n","  adding: ouput_model/pytorch_model.bin (deflated 7%)\n"]}],"source":["!zip -r ouput_model.zip ./ouput_model/"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"pegasus_finetune.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":5}
